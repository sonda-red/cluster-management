apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-14b-tp2
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels: { app.kubernetes.io/name: vllm-14b-tp2 }
  template:
    metadata:
      labels: { app.kubernetes.io/name: vllm-14b-tp2 }
    spec:
      nodeSelector:
        kubernetes.io/hostname: sonda-core
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "PreferNoSchedule"
      volumes:
      - name: modelkit
        persistentVolumeClaim:
          claimName: vllm-modelkits
      - name: dshm
        emptyDir: { medium: Memory, sizeLimit: 12Gi }
      initContainers:
      - name: kitops-init
        image: ghcr.io/kitops-ml/kitops-init:v1.7.0
        env:
        - name: MODELKIT_REF
          # swap to your 14B artifact tag
          value: "harbor.sonda.red.local/sonda-red/ds-r1-qwen-14b:0.0.1"
        - name: UNPACK_PATH
          value: /data
        - name: UNPACK_FILTER
          value: model
        - name: EXTRA_FLAGS
          value: "--tls-verify=false -i --concurrency 1 --progress fancy"
        volumeMounts: [{ name: modelkit, mountPath: /data }]
      containers:
      - name: vllm
        image: intelanalytics/ipex-llm-serving-xpu:latest
        command: ["/bin/bash","-lc"]
        args:
          - |
            source /opt/intel/1ccl-wks/setvars.sh && \
            python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \
              --served-model-name ds-r1-qwen-14b \
              --port 8014 \
              --model /data \
              --trust-remote-code \
              --device xpu \
              --dtype float16 \
              --tensor-parallel-size 2 \
              --gpu-memory-utilization 0.90 \
              --enforce-eager \
              --load-in-low-bit asym_int4 \
              --max-model-len 4096 \
              --max-num-batched-tokens 4096 \
              --max-num-seqs 24 \
              --block-size 8 \
              --api-key asdf \
              --distributed-executor-backend ray
        env:
          - { name: ZE_AFFINITY_MASK, value: "0,1" }
          - { name: ONEAPI_DEVICE_SELECTOR, value: "level_zero:gpu" }
          - { name: CCL_WORKER_COUNT, value: "2" }
          - { name: SYCL_CACHE_PERSISTENT, value: "1" }
          - { name: FI_PROVIDER, value: "shm" }
          - { name: CCL_ATL_TRANSPORT, value: "ofi" }
          - { name: CCL_ZE_IPC_EXCHANGE, value: "sockets" }
          - { name: CCL_ATL_SHM, value: "1" }
          - { name: USE_XETLA, value: "OFF" }
          - { name: SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS, value: "2" }
          - { name: TORCH_LLM_ALLREDUCE, value: "0" }
          - { name: CCL_SAME_STREAM, value: "1" }
          - { name: CCL_BLOCKING_WAIT, value: "0" }
        ports: [{ containerPort: 8014 }]
        volumeMounts:
          - { name: modelkit, mountPath: /data }
          - { name: dshm,   mountPath: /dev/shm }
        resources:
          requests: { cpu: "8", memory: "28Gi", gpu.intel.com/i915: 2 }
          limits:   { cpu: "16", memory: "36Gi", gpu.intel.com/i915: 2 }
        startupProbe:
          tcpSocket: { port: 8014 }
          periodSeconds: 10
          failureThreshold: 120
        readinessProbe:
          httpGet: { path: /v1/models, port: 8014 }
          periodSeconds: 5
          failureThreshold: 12
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-14b-tp2
  namespace: vllm
spec:
  selector: { app.kubernetes.io/name: vllm-14b-tp2 }
  ports: [{ name: http, port: 8014, targetPort: 8014 }]
  type: ClusterIP