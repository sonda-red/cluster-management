apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-14b-tp2
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels: { app.kubernetes.io/name: vllm-14b-tp2 }
  template:
    metadata:
      labels: { app.kubernetes.io/name: vllm-14b-tp2 }
    spec:
      nodeSelector:
        kubernetes.io/hostname: sonda-core
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "PreferNoSchedule"
      volumes:
      - name: modelkit
        persistentVolumeClaim:
          claimName: vllm-modelkits
      - name: dshm
        emptyDir: { medium: Memory, sizeLimit: 12Gi }
      initContainers:
      - name: kitops-init
        image: ghcr.io/kitops-ml/kitops-init:v1.7.0
        env:
        - name: MODELKIT_REF
          # swap to your 14B artifact tag
          value: "harbor.harbor/sonda-red/ds-r1-qwen-14b:0.0.1"
        - name: UNPACK_PATH
          value: /data/ds-r1-qwen-14b
        - name: UNPACK_FILTER
          value: model
        - name: EXTRA_FLAGS
          value: "--tls-verify=false -i --concurrency 1 --progress=fancy --plain-http -vv"
        volumeMounts: [{ name: modelkit, mountPath: /data/ds-r1-qwen-14b }]
      - name: check-two-gpus
        image: alpine:3.20
        command: ["/bin/sh","-lc"]
        args:
        - |
          set -euo pipefail
          ds=(/dev/dri/renderD*)
          [ ${#ds[@]} -ge 2 ] || { echo "Need 2 render nodes"; exit 1; }
          uniq=$(for d in "${ds[@]}"; do readlink -f "/sys/class/drm/$(basename $d)/device"; done | sort -u | wc -l)
          [ "$uniq" -ge 2 ] || { echo "Both shares map to same GPU"; exit 1; }
      containers:
      - name: vllm
        image: intelanalytics/ipex-llm-serving-xpu:latest
        command: ["/bin/bash","-lc"]
        args:
          - |
            source /opt/intel/1ccl-wks/setvars.sh && \
            python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \
              --served-model-name ds-r1-qwen-14b \
              --port 8014 \
              --model /data/ds-r1-qwen-14b \
              --trust-remote-code \
              --device xpu \
              --dtype float16 \ 
              --distributed-executor-backend mp \
              --tensor-parallel-size 2 \
              --gpu-memory-utilization 0.90 \
              --enforce-eager \
              --load-in-low-bit asym_int4 \
              --max-model-len 4096 \
              --max-num-batched-tokens 4096 \
              --max-num-seqs 24 \
        env:
         # - { name: ZE_AFFINITY_MASK, value: "0,1" }
          - { name: ONEAPI_DEVICE_SELECTOR, value: "level_zero:gpu" }
          #- { name: CCL_WORKER_COUNT, value: "2" }
          - { name: SYCL_CACHE_PERSISTENT, value: "1" }
          - { name: FI_PROVIDER, value: "shm" }
          - { name: CCL_ATL_TRANSPORT, value: "ofi" }
          - { name: CCL_ZE_IPC_EXCHANGE, value: "sockets" }
          - { name: USE_XETLA, value: "OFF" }
          - { name: SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS, value: "2" }
          - { name: TORCH_LLM_ALLREDUCE, value: "0" }
          - { name: CCL_SAME_STREAM, value: "1" }
          - { name: CCL_BLOCKING_WAIT, value: "0" }
          - { name: VLLM_WORKER_MULTIPROCESSING_METHOD, value: "spawn" }
          - { name: FI_PROVIDER, value: "tcp" }
          - { name: CCL_ATL_SHM, value: "0" }
          - { name: CCL_DG2_ALLREDUCE, value: "0" }
        ports: [{ containerPort: 8014 }]
        volumeMounts:
          - { name: modelkit, mountPath: /data/ds-r1-qwen-14b }
          - { name: dshm,   mountPath: /dev/shm }
        resources:
          requests: { cpu: "8", memory: "28Gi", gpu.intel.com/i915: 2 }
          limits:   { cpu: "16", memory: "36Gi", gpu.intel.com/i915: 2 }
        startupProbe:
          tcpSocket: { port: 8014 }
          periodSeconds: 10
          failureThreshold: 120
        readinessProbe:
          httpGet: { path: /v1/models, port: 8014 }
          periodSeconds: 5
          failureThreshold: 12
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-14b-tp2
  namespace: vllm
  labels:
    app.kubernetes.io/name: vllm-14b-tp2   # ‚Üê label on your Service
spec:
  selector: { app.kubernetes.io/name: vllm-14b-tp2 }
  ports: [{ name: http, port: 8014, targetPort: 8014 }]
  type: ClusterIP