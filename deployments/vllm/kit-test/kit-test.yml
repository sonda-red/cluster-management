apiVersion: apps/v1
kind: Deployment
metadata:
  name: kit-test
  namespace: vllm
spec:
  replicas: 1
  selector:
    matchLabels: { app.kubernetes.io/name: kit-test }
  template:
    metadata:
      labels: { app.kubernetes.io/name: kit-test }
    spec:
      nodeSelector:
        kubernetes.io/hostname: sonda-core
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "gpu"
          effect: "PreferNoSchedule"

      volumes:
      - name: modelkit
        persistentVolumeClaim:
          claimName: kit-test-modelkit
      # - name: models
      #   persistentVolumeClaim:
      #     claimName: kit-test-models
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 8Gi
      initContainers:
      - name: kitops-init
        image: ghcr.io/kitops-ml/kitops-init:v1.7.0
        env:
        - name: MODELKIT_REF
          value: "harbor.sonda.red.local/sonda-red/ds-r1-qwen-1.5b:0.0.1"
        - name: UNPACK_PATH
          value: /data
        - name: UNPACK_FILTER
          value: model
        - name: EXTRA_FLAGS
          value: "--tls-verify=false -i"
        volumeMounts: [{ name: modelkit, mountPath: /data }]
      containers:
        - name: vllm
          image: intelanalytics/ipex-llm-serving-xpu:latest
          # source oneCCL env then run the IPEX-LLM vLLM OpenAI server, like your script
          command: ["/bin/bash","-lc"]
          args:
            - |
              source /opt/intel/1ccl-wks/setvars.sh && \
              python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \
                --served-model-name ds-r1-qwen-1.5b \
                --port 8000 \
                --model /data \
                --trust-remote-code \
                --gpu-memory-utilization 0.8 \
                --device xpu \
                --dtype float16 \
                --enforce-eager \
                --max-model-len 1024 \
                --max-num-batched-tokens 1024 \
                --max-num-seqs 2 \
                --tensor-parallel-size 1
          env:
            - { name: CCL_WORKER_COUNT, value: "1" }
            - { name: SYCL_CACHE_PERSISTENT, value: "1" }
            - { name: FI_PROVIDER, value: "shm" }
            - { name: CCL_ATL_TRANSPORT, value: "ofi" }
            - { name: CCL_ZE_IPC_EXCHANGE, value: "sockets" }
            - { name: CCL_ATL_SHM, value: "1" }
            - { name: ONEAPI_DEVICE_SELECTOR, value: "level_zero:gpu" }
            - { name: USE_XETLA, value: "OFF" }
            - { name: SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS, value: "2" }
            - { name: TORCH_LLM_ALLREDUCE, value: "0" }
            - { name: CCL_SAME_STREAM, value: "1" }
            - { name: CCL_BLOCKING_WAIT, value: "0" }
          ports: [{ containerPort: 8000 }]
          volumeMounts:
            - { name: modelkit, mountPath: /data }
            # - { name: models, mountPath: /models }
            - { name: dshm,   mountPath: /dev/shm }
          resources:
            requests: { cpu: "4", memory: "12Gi", gpu.intel.com/i915: 1 }
            limits:   { cpu: "4", memory: "12Gi", gpu.intel.com/i915: 1 }
          startupProbe:
            tcpSocket: { port: 8000 }
            periodSeconds: 10
            failureThreshold: 120
          readinessProbe:
            httpGet: { path: /v1/models, port: 8000 }
            periodSeconds: 5
            failureThreshold: 12
---
apiVersion: v1
kind: Service
metadata:
  name: kit-test
  namespace: vllm
spec:
  selector: { app.kubernetes.io/name: kit-test }
  ports: [{ name: http, port: 8000, targetPort: 8000 }]
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kit-test-modelkit
  namespace: vllm
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: "local-path"
  resources:
    requests:
      storage: 10Gi
